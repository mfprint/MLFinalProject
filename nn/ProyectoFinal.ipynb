{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martinfigueroapadilla/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "from keras.initializers import glorot_normal\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 2 2 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martinfigueroapadilla/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# extract data from a csv\n",
    "# notice the cool options to skip lines at the beginning\n",
    "# and to only take data from certain columns\n",
    "training = np.genfromtxt('data/tweets.csv', delimiter=',', skip_header=1, usecols=(1, 3), dtype=None)\n",
    "\n",
    "# create our training data from the tweets\n",
    "train_x = [x[1] for x in training]\n",
    "# index all the sentiment labels\n",
    "train_y = np.asarray([x[0] for x in training])\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only work with the 3000 most popular words found in our dataset\n",
    "max_words = 5000\n",
    "\n",
    "# create a new Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "# feed our tweets to the Tokenizer\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "# Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "# Let's save this out so we can use it later\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)\n",
    "\n",
    "\n",
    "def convert_text_to_index_array(text):\n",
    "    # one really important thing that `text_to_word_sequence` does\n",
    "    # is make all texts the same length -- in this case, the length\n",
    "    # of the longest text in the set.\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]\n",
    "\n",
    "allWordIndices = []\n",
    "# for each tweet, change each token to its ID in the Tokenizer's word_index\n",
    "for text in train_x:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "# now we have a list of all tweets converted to index arrays.\n",
    "# cast as an array for future usage.\n",
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "# create one-hot matrices out of the indexed tweets\n",
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "# treat the labels as categories\n",
    "train_y = keras.utils.to_categorical(train_y, max(train_y) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = Sequential()\\nmodel.add(Dense(512, input_shape=(max_words,), activation='relu'))\\nmodel.add(Dropout(0.5))\\nmodel.add(Dense(256, activation='sigmoid'))\\nmodel.add(Dropout(0.5))\\nmodel.add(Dense(3, activation='softmax'))\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dimension = 3\n",
    "nHidden = 17\n",
    "nInputDimensions = train_x.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(nHidden, input_dim=nInputDimensions, activation='elu', kernel_initializer=glorot_normal(seed=0)))\n",
    "\n",
    "for i in range(0, 7): # capas\n",
    "    model.add(Dense(nHidden, activation='relu', kernel_initializer=glorot_normal(seed=0))) # agregar capa\n",
    "\n",
    "model.add(Dense(out_dimension, activation='sigmoid', kernel_initializer=glorot_normal(seed=0)))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8373 samples, validate on 931 samples\n",
      "Epoch 1/100\n",
      "8373/8373 [==============================] - 16s 2ms/step - loss: 0.4477 - acc: 0.7822 - val_loss: 0.7003 - val_acc: 0.6552\n",
      "Epoch 2/100\n",
      "8373/8373 [==============================] - 16s 2ms/step - loss: 0.2389 - acc: 0.8970 - val_loss: 0.8717 - val_acc: 0.6549\n",
      "Epoch 3/100\n",
      "8373/8373 [==============================] - 16s 2ms/step - loss: 0.1717 - acc: 0.9267 - val_loss: 1.2333 - val_acc: 0.5961\n",
      "Epoch 4/100\n",
      "8373/8373 [==============================] - 18s 2ms/step - loss: 0.1391 - acc: 0.9405 - val_loss: 1.7363 - val_acc: 0.5786\n",
      "Epoch 5/100\n",
      "8373/8373 [==============================] - 19s 2ms/step - loss: 0.1141 - acc: 0.9487 - val_loss: 1.5877 - val_acc: 0.6230\n",
      "Epoch 6/100\n",
      "8373/8373 [==============================] - 19s 2ms/step - loss: 0.0953 - acc: 0.9562 - val_loss: 2.5041 - val_acc: 0.5764\n",
      "Epoch 7/100\n",
      "8373/8373 [==============================] - 21s 3ms/step - loss: 0.0875 - acc: 0.9605 - val_loss: 2.5037 - val_acc: 0.5850\n",
      "Epoch 8/100\n",
      "8373/8373 [==============================] - 23s 3ms/step - loss: 0.0770 - acc: 0.9637 - val_loss: 2.4157 - val_acc: 0.5904\n",
      "Epoch 9/100\n",
      "8373/8373 [==============================] - 24s 3ms/step - loss: 0.0697 - acc: 0.9670 - val_loss: 3.1158 - val_acc: 0.5675\n",
      "Epoch 10/100\n",
      "8373/8373 [==============================] - 21s 3ms/step - loss: 0.0642 - acc: 0.9696 - val_loss: 3.3200 - val_acc: 0.5947\n",
      "Epoch 11/100\n",
      "8373/8373 [==============================] - 24s 3ms/step - loss: 0.0656 - acc: 0.9705 - val_loss: 3.0529 - val_acc: 0.6004\n",
      "Epoch 12/100\n",
      "8373/8373 [==============================] - 25s 3ms/step - loss: 0.0634 - acc: 0.9717 - val_loss: 2.7777 - val_acc: 0.6216\n",
      "Epoch 13/100\n",
      "8373/8373 [==============================] - 22s 3ms/step - loss: 0.0563 - acc: 0.9732 - val_loss: 3.2390 - val_acc: 0.6298\n",
      "Epoch 14/100\n",
      "8373/8373 [==============================] - 22s 3ms/step - loss: 0.0586 - acc: 0.9727 - val_loss: 3.3372 - val_acc: 0.5836\n",
      "Epoch 15/100\n",
      "8373/8373 [==============================] - 21s 3ms/step - loss: 0.0536 - acc: 0.9748 - val_loss: 3.2459 - val_acc: 0.6219\n",
      "Epoch 16/100\n",
      "8373/8373 [==============================] - 21s 3ms/step - loss: 0.0521 - acc: 0.9759 - val_loss: 3.5686 - val_acc: 0.5858\n",
      "Epoch 17/100\n",
      "8373/8373 [==============================] - 23s 3ms/step - loss: 0.0552 - acc: 0.9753 - val_loss: 3.7944 - val_acc: 0.5843\n",
      "Epoch 18/100\n",
      "8373/8373 [==============================] - 25s 3ms/step - loss: 0.0522 - acc: 0.9755 - val_loss: 2.9654 - val_acc: 0.6341\n",
      "Epoch 19/100\n",
      "8373/8373 [==============================] - 26s 3ms/step - loss: 0.0501 - acc: 0.9760 - val_loss: 4.0719 - val_acc: 0.5818\n",
      "Epoch 20/100\n",
      "8373/8373 [==============================] - 24s 3ms/step - loss: 0.0501 - acc: 0.9762 - val_loss: 3.6179 - val_acc: 0.6140\n",
      "Epoch 21/100\n",
      "8373/8373 [==============================] - 25s 3ms/step - loss: 0.0465 - acc: 0.9783 - val_loss: 3.6239 - val_acc: 0.6158\n",
      "Epoch 22/100\n",
      "8373/8373 [==============================] - 29s 4ms/step - loss: 0.0461 - acc: 0.9772 - val_loss: 4.2030 - val_acc: 0.5872\n",
      "Epoch 23/100\n",
      "8373/8373 [==============================] - 27s 3ms/step - loss: 0.0516 - acc: 0.9760 - val_loss: 3.3390 - val_acc: 0.6591\n",
      "Epoch 24/100\n",
      "8373/8373 [==============================] - 25s 3ms/step - loss: 0.0465 - acc: 0.9783 - val_loss: 4.3659 - val_acc: 0.5732\n",
      "Epoch 25/100\n",
      "8373/8373 [==============================] - 22s 3ms/step - loss: 0.0509 - acc: 0.9773 - val_loss: 3.6127 - val_acc: 0.6097\n",
      "Epoch 26/100\n",
      "8373/8373 [==============================] - 25s 3ms/step - loss: 0.0479 - acc: 0.9785 - val_loss: 3.3823 - val_acc: 0.6294\n",
      "Epoch 27/100\n",
      "8373/8373 [==============================] - 24s 3ms/step - loss: 0.0440 - acc: 0.9788 - val_loss: 4.0452 - val_acc: 0.5943\n",
      "Epoch 28/100\n",
      "8373/8373 [==============================] - 23s 3ms/step - loss: 0.0431 - acc: 0.9788 - val_loss: 3.6092 - val_acc: 0.6094\n",
      "Epoch 29/100\n",
      "8373/8373 [==============================] - 29s 3ms/step - loss: 0.0446 - acc: 0.9775 - val_loss: 3.7947 - val_acc: 0.6044\n",
      "Epoch 30/100\n",
      "8373/8373 [==============================] - 25s 3ms/step - loss: 0.0439 - acc: 0.9781 - val_loss: 3.9170 - val_acc: 0.6251\n",
      "Epoch 31/100\n",
      "8373/8373 [==============================] - 22s 3ms/step - loss: 0.0433 - acc: 0.9780 - val_loss: 4.2690 - val_acc: 0.5940\n",
      "Epoch 32/100\n",
      "8373/8373 [==============================] - 24s 3ms/step - loss: 0.0426 - acc: 0.9786 - val_loss: 4.0693 - val_acc: 0.6112\n",
      "Epoch 33/100\n",
      "8373/8373 [==============================] - 25s 3ms/step - loss: 0.0450 - acc: 0.9783 - val_loss: 4.0624 - val_acc: 0.6011\n",
      "Epoch 34/100\n",
      "8373/8373 [==============================] - 25s 3ms/step - loss: 0.0438 - acc: 0.9794 - val_loss: 3.4584 - val_acc: 0.6187\n",
      "Epoch 35/100\n",
      "8373/8373 [==============================] - 25s 3ms/step - loss: 0.0412 - acc: 0.9791 - val_loss: 4.1637 - val_acc: 0.6162\n",
      "Epoch 36/100\n",
      "8373/8373 [==============================] - 25s 3ms/step - loss: 0.0425 - acc: 0.9796 - val_loss: 4.1022 - val_acc: 0.6108\n",
      "Epoch 37/100\n",
      "8373/8373 [==============================] - 26s 3ms/step - loss: 0.0410 - acc: 0.9793 - val_loss: 4.3361 - val_acc: 0.6004\n",
      "Epoch 38/100\n",
      "8373/8373 [==============================] - 22s 3ms/step - loss: 0.0430 - acc: 0.9787 - val_loss: 3.4846 - val_acc: 0.6266\n",
      "Epoch 39/100\n",
      "8373/8373 [==============================] - 20s 2ms/step - loss: 0.0414 - acc: 0.9792 - val_loss: 4.1907 - val_acc: 0.6248\n",
      "Epoch 40/100\n",
      "8373/8373 [==============================] - 22s 3ms/step - loss: 0.0415 - acc: 0.9792 - val_loss: 3.7802 - val_acc: 0.6219\n",
      "Epoch 41/100\n",
      "8373/8373 [==============================] - 22s 3ms/step - loss: 0.0403 - acc: 0.9798 - val_loss: 4.3053 - val_acc: 0.6187\n",
      "Epoch 42/100\n",
      "8373/8373 [==============================] - 26s 3ms/step - loss: 0.0426 - acc: 0.9784 - val_loss: 3.9826 - val_acc: 0.6062\n",
      "Epoch 43/100\n",
      "8373/8373 [==============================] - 26s 3ms/step - loss: 0.0407 - acc: 0.9797 - val_loss: 4.0667 - val_acc: 0.6305\n",
      "Epoch 44/100\n",
      "8373/8373 [==============================] - 24s 3ms/step - loss: 0.0442 - acc: 0.9785 - val_loss: 3.8854 - val_acc: 0.5990\n",
      "Epoch 45/100\n",
      "8373/8373 [==============================] - 25s 3ms/step - loss: 0.0419 - acc: 0.9789 - val_loss: 3.9463 - val_acc: 0.6205\n",
      "Epoch 46/100\n",
      "8373/8373 [==============================] - 24s 3ms/step - loss: 0.0405 - acc: 0.9796 - val_loss: 4.1021 - val_acc: 0.6069\n",
      "Epoch 47/100\n",
      "8373/8373 [==============================] - 24s 3ms/step - loss: 0.0397 - acc: 0.9797 - val_loss: 4.3783 - val_acc: 0.6101\n",
      "Epoch 48/100\n",
      "8373/8373 [==============================] - 21s 2ms/step - loss: 0.0411 - acc: 0.9794 - val_loss: 4.1120 - val_acc: 0.6255\n",
      "Epoch 49/100\n",
      "8373/8373 [==============================] - 18s 2ms/step - loss: 0.0423 - acc: 0.9791 - val_loss: 4.3750 - val_acc: 0.5943\n",
      "Epoch 50/100\n",
      "8373/8373 [==============================] - 18s 2ms/step - loss: 0.0425 - acc: 0.9794 - val_loss: 3.3782 - val_acc: 0.6434\n",
      "Epoch 51/100\n",
      "8373/8373 [==============================] - 17s 2ms/step - loss: 0.0397 - acc: 0.9789 - val_loss: 3.7621 - val_acc: 0.6237\n",
      "Epoch 52/100\n",
      "8373/8373 [==============================] - 16s 2ms/step - loss: 0.0395 - acc: 0.9796 - val_loss: 4.0146 - val_acc: 0.6205\n",
      "Epoch 53/100\n",
      "8373/8373 [==============================] - 16s 2ms/step - loss: 0.0394 - acc: 0.9795 - val_loss: 4.1690 - val_acc: 0.6266\n",
      "Epoch 54/100\n",
      "8373/8373 [==============================] - 17s 2ms/step - loss: 0.0416 - acc: 0.9793 - val_loss: 3.7647 - val_acc: 0.6445\n",
      "Epoch 55/100\n",
      "8373/8373 [==============================] - 19s 2ms/step - loss: 0.0408 - acc: 0.9799 - val_loss: 3.4844 - val_acc: 0.6373\n",
      "Epoch 56/100\n",
      "8373/8373 [==============================] - 18s 2ms/step - loss: 0.0402 - acc: 0.9789 - val_loss: 3.9169 - val_acc: 0.6201\n",
      "Epoch 57/100\n",
      "8373/8373 [==============================] - 17s 2ms/step - loss: 0.0394 - acc: 0.9795 - val_loss: 3.8573 - val_acc: 0.6126\n",
      "Epoch 58/100\n",
      "8373/8373 [==============================] - 16s 2ms/step - loss: 0.0397 - acc: 0.9796 - val_loss: 4.1277 - val_acc: 0.6173\n",
      "Epoch 59/100\n",
      "8373/8373 [==============================] - 16s 2ms/step - loss: 0.0408 - acc: 0.9801 - val_loss: 4.3436 - val_acc: 0.6001\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8373/8373 [==============================] - 16s 2ms/step - loss: 0.0391 - acc: 0.9795 - val_loss: 4.4179 - val_acc: 0.6169\n",
      "Epoch 61/100\n",
      "8373/8373 [==============================] - 17s 2ms/step - loss: 0.0389 - acc: 0.9803 - val_loss: 4.1424 - val_acc: 0.6244\n",
      "Epoch 62/100\n",
      "8373/8373 [==============================] - 17s 2ms/step - loss: 0.0405 - acc: 0.9799 - val_loss: 4.2081 - val_acc: 0.5926\n",
      "Epoch 63/100\n",
      "8373/8373 [==============================] - 16s 2ms/step - loss: 0.0420 - acc: 0.9797 - val_loss: 3.8012 - val_acc: 0.6319\n",
      "Epoch 64/100\n",
      "8373/8373 [==============================] - 18s 2ms/step - loss: 0.0388 - acc: 0.9801 - val_loss: 4.0440 - val_acc: 0.6219\n",
      "Epoch 65/100\n",
      "8373/8373 [==============================] - 18s 2ms/step - loss: 0.0389 - acc: 0.9801 - val_loss: 4.0247 - val_acc: 0.6348\n",
      "Epoch 66/100\n",
      "8373/8373 [==============================] - 21s 2ms/step - loss: 0.0390 - acc: 0.9803 - val_loss: 4.1436 - val_acc: 0.6301\n",
      "Epoch 67/100\n",
      "8373/8373 [==============================] - 17s 2ms/step - loss: 0.0402 - acc: 0.9804 - val_loss: 4.0796 - val_acc: 0.6008\n",
      "Epoch 68/100\n",
      "8373/8373 [==============================] - 17s 2ms/step - loss: 0.0407 - acc: 0.9795 - val_loss: 4.5072 - val_acc: 0.5865\n",
      "Epoch 69/100\n",
      "8373/8373 [==============================] - 19s 2ms/step - loss: 0.0382 - acc: 0.9800 - val_loss: 4.5722 - val_acc: 0.6090\n",
      "Epoch 70/100\n",
      "8373/8373 [==============================] - 18s 2ms/step - loss: 0.0392 - acc: 0.9800 - val_loss: 4.3304 - val_acc: 0.6094\n",
      "Epoch 71/100\n",
      "8373/8373 [==============================] - 18s 2ms/step - loss: 0.0394 - acc: 0.9798 - val_loss: 4.3562 - val_acc: 0.5868\n",
      "Epoch 72/100\n",
      "8373/8373 [==============================] - 17s 2ms/step - loss: 0.0393 - acc: 0.9800 - val_loss: 4.3131 - val_acc: 0.6115\n",
      "Epoch 73/100\n",
      "8373/8373 [==============================] - 16s 2ms/step - loss: 0.0407 - acc: 0.9803 - val_loss: 3.8444 - val_acc: 0.6251\n",
      "Epoch 74/100\n",
      "8373/8373 [==============================] - 16s 2ms/step - loss: 0.0410 - acc: 0.9806 - val_loss: 4.2223 - val_acc: 0.6033\n",
      "Epoch 75/100\n",
      "8373/8373 [==============================] - 16s 2ms/step - loss: 0.0384 - acc: 0.9800 - val_loss: 3.9488 - val_acc: 0.6266\n",
      "Epoch 76/100\n",
      "8373/8373 [==============================] - 16s 2ms/step - loss: 0.0386 - acc: 0.9794 - val_loss: 4.4110 - val_acc: 0.6130\n",
      "Epoch 77/100\n",
      "8373/8373 [==============================] - 17s 2ms/step - loss: 0.0381 - acc: 0.9799 - val_loss: 4.2124 - val_acc: 0.6412\n",
      "Epoch 78/100\n",
      "8373/8373 [==============================] - 19s 2ms/step - loss: 0.0392 - acc: 0.9799 - val_loss: 4.6911 - val_acc: 0.6087\n",
      "Epoch 79/100\n",
      "8373/8373 [==============================] - 19s 2ms/step - loss: 0.0411 - acc: 0.9795 - val_loss: 4.5189 - val_acc: 0.5958\n",
      "Epoch 80/100\n",
      "8373/8373 [==============================] - 20s 2ms/step - loss: 0.0406 - acc: 0.9801 - val_loss: 3.9819 - val_acc: 0.6269\n",
      "Epoch 81/100\n",
      "8373/8373 [==============================] - 20s 2ms/step - loss: 0.0423 - acc: 0.9791 - val_loss: 4.0091 - val_acc: 0.6412\n",
      "Epoch 82/100\n",
      "8373/8373 [==============================] - 19s 2ms/step - loss: 0.0407 - acc: 0.9795 - val_loss: 4.4762 - val_acc: 0.6079\n",
      "Epoch 83/100\n",
      "8373/8373 [==============================] - 19s 2ms/step - loss: 0.0405 - acc: 0.9802 - val_loss: 3.9963 - val_acc: 0.6416\n",
      "Epoch 84/100\n",
      "8373/8373 [==============================] - 18s 2ms/step - loss: 0.0378 - acc: 0.9804 - val_loss: 4.5108 - val_acc: 0.6198\n",
      "Epoch 85/100\n",
      "8373/8373 [==============================] - 17s 2ms/step - loss: 0.0385 - acc: 0.9798 - val_loss: 4.3416 - val_acc: 0.6316\n",
      "Epoch 86/100\n",
      "8373/8373 [==============================] - 17s 2ms/step - loss: 0.0393 - acc: 0.9801 - val_loss: 4.5910 - val_acc: 0.6126\n",
      "Epoch 87/100\n",
      "8373/8373 [==============================] - 19s 2ms/step - loss: 0.0396 - acc: 0.9801 - val_loss: 4.5110 - val_acc: 0.6280\n",
      "Epoch 88/100\n",
      "8373/8373 [==============================] - 18s 2ms/step - loss: 0.0411 - acc: 0.9798 - val_loss: 3.3504 - val_acc: 0.6763\n",
      "Epoch 89/100\n",
      "8373/8373 [==============================] - 19s 2ms/step - loss: 0.0393 - acc: 0.9805 - val_loss: 4.2942 - val_acc: 0.6230\n",
      "Epoch 90/100\n",
      "8373/8373 [==============================] - 18s 2ms/step - loss: 0.0378 - acc: 0.9805 - val_loss: 4.4510 - val_acc: 0.6248\n",
      "Epoch 91/100\n",
      "8373/8373 [==============================] - 18s 2ms/step - loss: 0.0378 - acc: 0.9804 - val_loss: 4.7990 - val_acc: 0.6248\n",
      "Epoch 92/100\n",
      "8373/8373 [==============================] - 17s 2ms/step - loss: 0.0391 - acc: 0.9804 - val_loss: 4.7846 - val_acc: 0.6108\n",
      "Epoch 93/100\n",
      "8373/8373 [==============================] - 16s 2ms/step - loss: 0.0393 - acc: 0.9793 - val_loss: 3.8366 - val_acc: 0.6477\n",
      "Epoch 94/100\n",
      "8373/8373 [==============================] - 17s 2ms/step - loss: 0.0404 - acc: 0.9797 - val_loss: 4.3520 - val_acc: 0.6165\n",
      "Epoch 95/100\n",
      "8373/8373 [==============================] - 18s 2ms/step - loss: 0.0386 - acc: 0.9802 - val_loss: 4.1313 - val_acc: 0.6309\n",
      "Epoch 96/100\n",
      "8373/8373 [==============================] - 17s 2ms/step - loss: 0.0387 - acc: 0.9804 - val_loss: 4.3993 - val_acc: 0.6312\n",
      "Epoch 97/100\n",
      "8373/8373 [==============================] - 18s 2ms/step - loss: 0.0383 - acc: 0.9802 - val_loss: 3.7584 - val_acc: 0.6477\n",
      "Epoch 98/100\n",
      "8373/8373 [==============================] - 17s 2ms/step - loss: 0.0399 - acc: 0.9802 - val_loss: 4.3159 - val_acc: 0.6266\n",
      "Epoch 99/100\n",
      "8373/8373 [==============================] - 16s 2ms/step - loss: 0.0380 - acc: 0.9803 - val_loss: 4.2155 - val_acc: 0.6423\n",
      "Epoch 100/100\n",
      "8373/8373 [==============================] - 16s 2ms/step - loss: 0.0391 - acc: 0.9801 - val_loss: 4.4244 - val_acc: 0.6115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb33f677f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, batch_size=5, epochs=100, verbose=1, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open('model.h5', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
